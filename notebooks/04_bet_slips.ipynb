{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is work in progress and not completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from itertools import combinations, product\n",
    "from recsys.utils.utils import IDConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetslipDataset(Dataset):\n",
    "    def __init__(self, user_df, event_df, bets_df, market_df, max_selections=5):\n",
    "        self.max_selections = max_selections\n",
    "        \n",
    "        # Create ID mappings first\n",
    "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(user_df['player_id'].unique())}\n",
    "        self.event_id_to_idx = {eid: idx for idx, eid in enumerate(event_df['event_id'].unique())}\n",
    "        self.market_id_to_idx = {mid: idx for idx, mid in enumerate(market_df['market_id'].unique())}\n",
    "        \n",
    "        # Filter bets to include only valid IDs\n",
    "        valid_bets_df = bets_df[\n",
    "            (bets_df['player_id'].isin(self.user_id_to_idx.keys())) &\n",
    "            (bets_df['event_id'].isin(self.event_id_to_idx.keys())) &\n",
    "            (bets_df['market_id'].isin(self.market_id_to_idx.keys()))\n",
    "        ]\n",
    "        \n",
    "        print(f\"Original bets: {len(bets_df)}, Valid bets: {len(valid_bets_df)}\")\n",
    "        \n",
    "        # Process features\n",
    "        self.user_features = self.process_user_features(user_df, valid_bets_df)\n",
    "        self.event_features = self.process_event_features(event_df)\n",
    "        self.market_features = self.process_market_features(market_df)\n",
    "        \n",
    "        # Create betslips from valid bets\n",
    "        self.betslips = self.create_betslips(valid_bets_df)\n",
    "        \n",
    "        print(f\"Processed {len(self.betslips)} valid betslips\")\n",
    "        print(f\"User features shape: {self.user_features.shape}\")\n",
    "        print(f\"Event features shape: {self.event_features.shape}\")\n",
    "        print(f\"Market features shape: {self.market_features.shape}\")\n",
    "\n",
    "    def process_user_features(self, users_df, bets_df):\n",
    "        # Calculate user betting patterns\n",
    "        user_stats = bets_df.groupby('player_id').agg({\n",
    "            'amount': ['mean', 'count'],\n",
    "            'bet_odds': 'mean',\n",
    "            'status': lambda x: (x == 'won').mean()\n",
    "        }).reset_index()\n",
    "        \n",
    "        user_stats.columns = ['player_id', 'avg_stake', 'bet_count', 'avg_odds', 'win_rate']\n",
    "        \n",
    "        # Create feature matrix for all users in mapping\n",
    "        feature_matrix = np.zeros((len(self.user_id_to_idx), 4))  # 4 features\n",
    "        \n",
    "        for _, row in user_stats.iterrows():\n",
    "            if row['player_id'] in self.user_id_to_idx:\n",
    "                idx = self.user_id_to_idx[row['player_id']]\n",
    "                feature_matrix[idx] = [\n",
    "                    row['avg_stake'], row['bet_count'],\n",
    "                    row['avg_odds'], row['win_rate']\n",
    "                ]\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def process_event_features(self, events_df):\n",
    "        # Calculate temporal features\n",
    "        events_df['time_to_event'] = (\n",
    "            pd.to_datetime(events_df['start_time']) - pd.to_datetime('now')\n",
    "        ).dt.total_seconds() / 3600\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_matrix = np.zeros((len(self.event_id_to_idx), 3))  # time + sport + league\n",
    "        \n",
    "        for event_id, idx in self.event_id_to_idx.items():\n",
    "            event = events_df[events_df['event_id'] == event_id].iloc[0]\n",
    "            feature_matrix[idx] = [\n",
    "                event['time_to_event'],\n",
    "                hash(str(event['sport_id'])) % 100,  # Simple hash for categorical\n",
    "                hash(str(event['league_id'])) % 100\n",
    "            ]\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def process_market_features(self, market_df):\n",
    "        # Create feature matrix\n",
    "        feature_matrix = np.zeros((len(self.market_id_to_idx), 1))  # odds only\n",
    "        \n",
    "        for market_id, idx in self.market_id_to_idx.items():\n",
    "            market = market_df[market_df['market_id'] == market_id].iloc[0]\n",
    "            feature_matrix[idx] = [market['avg_odds']]\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "    def create_betslips(self, bets_df):\n",
    "        betslips = []\n",
    "        \n",
    "        for bet_id, group in bets_df.groupby('bet_id'):\n",
    "            n_selections = len(group)\n",
    "            if n_selections > self.max_selections:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                betslip = {\n",
    "                    'player_id': group['player_id'].iloc[0],\n",
    "                    'event_ids': group['event_id'].tolist(),\n",
    "                    'market_ids': group['market_id'].tolist(),\n",
    "                    'odds': group['bet_odds'].tolist(),\n",
    "                    'combined_odds': group['bet_odds'].prod(),\n",
    "                    'n_selections': n_selections,\n",
    "                    'label': 1.0 if group['status'].iloc[0] == 'won' else 0.0\n",
    "                }\n",
    "                betslips.append(betslip)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating betslip for bet_id {bet_id}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return betslips\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.betslips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        betslip = self.betslips[idx]\n",
    "        \n",
    "        try:\n",
    "            # Get user features\n",
    "            user_idx = self.user_id_to_idx[betslip['player_id']]\n",
    "            user_features = self.user_features[user_idx]\n",
    "            \n",
    "            # Pad event and market features\n",
    "            event_features = np.zeros((self.max_selections, self.event_features.shape[1]))\n",
    "            market_features = np.zeros((self.max_selections, self.market_features.shape[1]))\n",
    "            \n",
    "            for i, (event_id, market_id) in enumerate(zip(betslip['event_ids'], betslip['market_ids'])):\n",
    "                event_idx = self.event_id_to_idx[event_id]\n",
    "                market_idx = self.market_id_to_idx[market_id]\n",
    "                \n",
    "                event_features[i] = self.event_features[event_idx]\n",
    "                market_features[i] = self.market_features[market_idx]\n",
    "            \n",
    "            return {\n",
    "                'user_features': torch.FloatTensor(user_features),\n",
    "                'event_features': torch.FloatTensor(event_features),\n",
    "                'market_features': torch.FloatTensor(market_features),\n",
    "                'combined_odds': torch.FloatTensor([betslip['combined_odds']]),\n",
    "                'n_selections': torch.LongTensor([betslip['n_selections']]),\n",
    "                'label': torch.FloatTensor([betslip['label']])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing betslip {idx}: {str(e)}\")\n",
    "            # Return zero tensors as fallback\n",
    "            return {\n",
    "                'user_features': torch.zeros(self.user_features.shape[1]),\n",
    "                'event_features': torch.zeros(self.max_selections, self.event_features.shape[1]),\n",
    "                'market_features': torch.zeros(self.max_selections, self.market_features.shape[1]),\n",
    "                'combined_odds': torch.zeros(1),\n",
    "                'n_selections': torch.zeros(1, dtype=torch.long),\n",
    "                'label': torch.zeros(1)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetslipRecommenderModel(nn.Module):\n",
    "    def __init__(self, user_dim, event_dim, market_dim, embedding_dim=64, max_selections=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "        self.max_selections = max_selections\n",
    "        \n",
    "        # User Tower\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(user_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "        self.user_tower.apply(init_weights)\n",
    "        \n",
    "        # Event Tower\n",
    "        self.event_tower = nn.Sequential(\n",
    "            nn.Linear(event_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "        self.event_tower.apply(init_weights)\n",
    "        \n",
    "        # Market Tower\n",
    "        self.market_tower = nn.Sequential(\n",
    "            nn.Linear(market_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "        self.market_tower.apply(init_weights)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.predictor.apply(init_weights)\n",
    "\n",
    "    def forward(self, user_features, event_features, market_features, combined_odds, n_selections):\n",
    "        batch_size = user_features.size(0)\n",
    "        \n",
    "        # Add small noise to prevent overfitting\n",
    "        if self.training:\n",
    "            user_features = user_features + torch.randn_like(user_features) * 0.01\n",
    "            event_features = event_features + torch.randn_like(event_features) * 0.01\n",
    "            market_features = market_features + torch.randn_like(market_features) * 0.01\n",
    "        \n",
    "        # Process features\n",
    "        user_emb = self.user_tower(user_features)\n",
    "        \n",
    "        # Process events and markets\n",
    "        event_features_flat = event_features.view(-1, event_features.size(-1))\n",
    "        market_features_flat = market_features.view(-1, market_features.size(-1))\n",
    "        \n",
    "        event_emb = self.event_tower(event_features_flat)\n",
    "        market_emb = self.market_tower(market_features_flat)\n",
    "        \n",
    "        # Reshape back\n",
    "        event_emb = event_emb.view(batch_size, self.max_selections, -1)\n",
    "        market_emb = market_emb.view(batch_size, self.max_selections, -1)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        selection_emb = event_emb + market_emb\n",
    "        \n",
    "        # Average pooling over selections\n",
    "        betslip_emb = torch.mean(selection_emb, dim=1)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([\n",
    "            betslip_emb + user_emb,\n",
    "            combined_odds,\n",
    "            n_selections.float()\n",
    "        ], dim=1)\n",
    "        \n",
    "        return self.predictor(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_betslip_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Custom loss with regularization\n",
    "    def custom_loss(outputs, labels, model):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(outputs, labels)\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_lambda = 0.01\n",
    "        l2_reg = torch.tensor(0.).to(device)\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        \n",
    "        return bce_loss + l2_lambda * l2_reg\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            user_features = batch['user_features'].to(device)\n",
    "            event_features = batch['event_features'].to(device)\n",
    "            market_features = batch['market_features'].to(device)\n",
    "            combined_odds = batch['combined_odds'].to(device)\n",
    "            n_selections = batch['n_selections'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                user_features,\n",
    "                event_features,\n",
    "                market_features,\n",
    "                combined_odds,\n",
    "                n_selections\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = custom_loss(outputs, labels, model)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Batch {batch_idx}:')\n",
    "                print(f'Loss: {loss.item():.4f}')\n",
    "                print(f'Accuracy: {100 * train_correct/train_total:.2f}%')\n",
    "                print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user_features = batch['user_features'].to(device)\n",
    "                event_features = batch['event_features'].to(device)\n",
    "                market_features = batch['market_features'].to(device)\n",
    "                combined_odds = batch['combined_odds'].to(device)\n",
    "                n_selections = batch['n_selections'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    user_features,\n",
    "                    event_features,\n",
    "                    market_features,\n",
    "                    combined_odds,\n",
    "                    n_selections\n",
    "                )\n",
    "                \n",
    "                loss = custom_loss(outputs, labels, model)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Train Accuracy: {100 * train_correct/train_total:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        print(f\"Val Accuracy: {100 * val_correct/val_total:.2f}%\")\n",
    "        print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_betslip_recommendations(\n",
    "    model, user_id, available_events, market_df, dataset,\n",
    "    top_k=5, max_selections=3, min_odds=1.5, max_combined_odds=10.0\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Preparing to generate recommendations...\")\n",
    "    \n",
    "    try:\n",
    "        # Get user features\n",
    "        user_idx = dataset.user_id_to_idx[user_id]\n",
    "        user_features = dataset.user_features[user_idx]\n",
    "        user_features = torch.FloatTensor(user_features).unsqueeze(0).to(device)\n",
    "        \n",
    "        candidate_betslips = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Single selections\n",
    "            for event_id in available_events:\n",
    "                if event_id not in dataset.event_id_to_idx:\n",
    "                    continue\n",
    "                    \n",
    "                for market_id in dataset.market_id_to_idx.keys():\n",
    "                    # Initialize feature arrays\n",
    "                    event_features = np.zeros((dataset.max_selections, dataset.event_features.shape[1]))\n",
    "                    market_features = np.zeros((dataset.max_selections, dataset.market_features.shape[1]))\n",
    "                    \n",
    "                    # Fill first selection\n",
    "                    event_idx = dataset.event_id_to_idx[event_id]\n",
    "                    market_idx = dataset.market_id_to_idx[market_id]\n",
    "                    \n",
    "                    event_features[0] = dataset.event_features[event_idx]\n",
    "                    market_features[0] = dataset.market_features[market_idx]\n",
    "                    \n",
    "                    # Get odds\n",
    "                    odds = float(market_df.loc[market_df['market_id'] == market_id, 'avg_odds'].iloc[0])\n",
    "                    if odds < min_odds:\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare model inputs\n",
    "                    event_features = torch.FloatTensor(event_features).unsqueeze(0).to(device)\n",
    "                    market_features = torch.FloatTensor(market_features).unsqueeze(0).to(device)\n",
    "                    combined_odds = torch.FloatTensor([[odds]]).to(device)\n",
    "                    n_selections = torch.LongTensor([[1]]).to(device)\n",
    "                    \n",
    "                    # Get prediction\n",
    "                    score = model(\n",
    "                        user_features,\n",
    "                        event_features,\n",
    "                        market_features,\n",
    "                        combined_odds,\n",
    "                        n_selections\n",
    "                    )\n",
    "                    \n",
    "                    candidate_betslips.append({\n",
    "                        'events': [event_id],\n",
    "                        'markets': [market_id],\n",
    "                        'odds': [odds],\n",
    "                        'combined_odds': odds,\n",
    "                        'score': score.item()\n",
    "                    })\n",
    "            \n",
    "            # Multiple selections\n",
    "            for n_sel in range(2, max_selections + 1):\n",
    "                print(f\"Generating {n_sel}-selection betslips...\")\n",
    "                \n",
    "                for events in combinations(available_events, n_sel):\n",
    "                    # Validate events\n",
    "                    if not all(e in dataset.event_id_to_idx for e in events):\n",
    "                        continue\n",
    "                    \n",
    "                    # Get markets for each event\n",
    "                    markets_list = [dataset.market_id_to_idx.keys() for _ in range(n_sel)]\n",
    "                    \n",
    "                    for markets in product(*markets_list):\n",
    "                        # Get odds\n",
    "                        try:\n",
    "                            odds = [\n",
    "                                float(market_df.loc[market_df['market_id'] == m, 'odds'].iloc[0])\n",
    "                                for m in markets\n",
    "                            ]\n",
    "                        except (IndexError, KeyError):\n",
    "                            continue\n",
    "                        \n",
    "                        combined_odds = np.prod(odds)\n",
    "                        if combined_odds > max_combined_odds or min(odds) < min_odds:\n",
    "                            continue\n",
    "                        \n",
    "                        # Prepare features\n",
    "                        event_features = np.zeros((dataset.max_selections, dataset.event_features.shape[1]))\n",
    "                        market_features = np.zeros((dataset.max_selections, dataset.market_features.shape[1]))\n",
    "                        \n",
    "                        # Fill features\n",
    "                        for i, (event_id, market_id) in enumerate(zip(events, markets)):\n",
    "                            event_idx = dataset.event_id_to_idx[event_id]\n",
    "                            market_idx = dataset.market_id_to_idx[market_id]\n",
    "                            \n",
    "                            event_features[i] = dataset.event_features[event_idx]\n",
    "                            market_features[i] = dataset.market_features[market_idx]\n",
    "                        \n",
    "                        # Prepare model inputs\n",
    "                        event_features = torch.FloatTensor(event_features).unsqueeze(0).to(device)\n",
    "                        market_features = torch.FloatTensor(market_features).unsqueeze(0).to(device)\n",
    "                        combined_odds_tensor = torch.FloatTensor([[combined_odds]]).to(device)\n",
    "                        n_selections_tensor = torch.LongTensor([[n_sel]]).to(device)\n",
    "                        \n",
    "                        # Get prediction\n",
    "                        score = model(\n",
    "                            user_features,\n",
    "                            event_features,\n",
    "                            market_features,\n",
    "                            combined_odds_tensor,\n",
    "                            n_selections_tensor\n",
    "                        )\n",
    "                        \n",
    "                        candidate_betslips.append({\n",
    "                            'events': list(events),\n",
    "                            'markets': list(markets),\n",
    "                            'odds': odds,\n",
    "                            'combined_odds': combined_odds,\n",
    "                            'score': score.item()\n",
    "                        })\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        sorted_betslips = sorted(candidate_betslips, key=lambda x: x['score'], reverse=True)\n",
    "        return sorted_betslips[:top_k]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recommendations: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def get_betslip_details(betslip, event_df, market_df):\n",
    "    events_info = []\n",
    "    for event_id in betslip['events']:\n",
    "        event = event_df[event_df['event_id'] == event_id].iloc[0]\n",
    "        events_info.append(f\"{event['home_team']} vs {event['away_team']}\")\n",
    "    \n",
    "    markets_info = []\n",
    "    for market_id in betslip['markets']:\n",
    "        market = market_df[market_df['market_id'] == market_id].iloc[0]\n",
    "        markets_info.append(f\"{market['market_type']}\")\n",
    "    \n",
    "    return events_info, markets_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from recsys.config import Settings\n",
    "\n",
    "settings = Settings()\n",
    "SOURCE_DIR = settings.SOURCE_DATA_DIR\n",
    "processed_dir = settings.PROCESSED_DATA_DIR\n",
    "# Prepare data\n",
    "user_df = pd.read_csv(SOURCE_DIR / 'users.csv')\n",
    "event_df = pd.read_csv(SOURCE_DIR / 'events.csv')\n",
    "bet_df = pd.read_csv(SOURCE_DIR / 'bets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_events = set(bet_df.event_id.to_list())\n",
    "evnts = set(event_df.event_id.to_list())\n",
    "list_of_events = evnts.union(bet_events)\n",
    "event_id_converter = IDConverter()\n",
    "for id in list_of_events:\n",
    "    event_id_converter.convert(id)\n",
    "player_id_converter = IDConverter()\n",
    "user_df['player_id'] = user_df['player_id'].apply(player_id_converter.convert)\n",
    "brand_id_converter = IDConverter()\n",
    "user_df['brand_id'] = user_df['brand_id'].apply(brand_id_converter.convert)\n",
    "# bet_id, outcome_id, market_id\n",
    "bet_id_converter = IDConverter()\n",
    "bet_df['bet_id'] = bet_df['bet_id'].apply(bet_id_converter.convert)\n",
    "outcome_id_converter = IDConverter()\n",
    "bet_df['outcome_id'] = bet_df['outcome_id'].apply(outcome_id_converter.convert)\n",
    "market_id_converter = IDConverter()\n",
    "bet_df['market_id'] = bet_df['market_id'].apply(market_id_converter.convert)\n",
    "\n",
    "event_df['event_id'] = event_df['event_id'].map(event_id_converter.id_to_int)\n",
    "sport_id_converter = IDConverter()\n",
    "event_df['sport_id'] = event_df['sport_id'].apply(sport_id_converter.convert)\n",
    "league_id_converter = IDConverter()\n",
    "event_df['league_id'] = event_df['league_id'].apply(league_id_converter.convert)\n",
    "bet_df.player_id = bet_df.player_id.map(player_id_converter.id_to_int)\n",
    "bet_df.brand_id = bet_df.brand_id.map(brand_id_converter.id_to_int)\n",
    "bet_df.event_id = bet_df.event_id.map(event_id_converter.id_to_int)\n",
    "user_df = user_df.drop_duplicates().copy(deep=True)\n",
    "event_df = event_df.drop_duplicates().copy(deep=True)\n",
    "bet_df = bet_df.drop_duplicates().copy(deep= True)\n",
    "user_df.player_reg_date = user_df.player_reg_date.str[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_df['win_lose'] = bet_df.status.map({'win': 1, 'lose': 0})\n",
    "bet_df.bet_date = pd.to_datetime(bet_df.bet_date.str[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "market_cols = ['market_id','win_lose', 'outcome_odds', 'bet_id']\n",
    "market_df = bet_df[market_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df = market_df.groupby(['market_id']).agg({'bet_id': 'count', 'win_lose': 'mean', 'outcome_odds' : 'mean' }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df.columns = ['market_id', 'bet_count', 'win_rate', 'avg_odds' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "user_df = user_df.copy()\n",
    "event_df = event_df.copy()\n",
    "bets_df = bet_df.copy()\n",
    "market_df = market_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200798 betslips\n",
      "User features shape: (25621, 9)\n",
      "Event features shape: (59387, 3)\n",
      "Market features shape: (954, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataset\n",
    "dataset = BetslipDataset(user_df, event_df, bets_df, market_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data loaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "def initialize_model(dataset):\n",
    "    user_dim = dataset.user_features.shape[1]  # Number of user features\n",
    "    event_dim = dataset.event_features.shape[1]  # Number of event features\n",
    "    market_dim = dataset.market_features.shape[1]  # Number of market features\n",
    "    \n",
    "    print(f\"User dimension: {user_dim}\")\n",
    "    print(f\"Event dimension: {event_dim}\")\n",
    "    print(f\"Market dimension: {market_dim}\")\n",
    "    \n",
    "    model = BetslipRecommenderModel(\n",
    "        user_dim=user_dim,\n",
    "        event_dim=event_dim,\n",
    "        market_dim=market_dim,\n",
    "        embedding_dim=64,\n",
    "        max_selections=dataset.max_selections\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data distribution...\n",
      "Data Distribution:\n",
      "Positive samples: 0.0\n",
      "Total samples: 36172\n",
      "Positive ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check data distribution\n",
    "def analyze_data(train_loader):\n",
    "    positive_samples = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        labels = batch['label']\n",
    "        positive_samples += labels.sum().item()\n",
    "        total_samples += len(labels)\n",
    "    \n",
    "    print(f\"Data Distribution:\")\n",
    "    print(f\"Positive samples: {positive_samples}\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Positive ratio: {100 * positive_samples/total_samples:.2f}%\")\n",
    "\n",
    "# Usage\n",
    "print(\"Analyzing data distribution...\")\n",
    "analyze_data(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nStarting training...\")\n",
    "model = initialize_model(dataset)\n",
    "train_betslip_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "model = initialize_model(dataset)\n",
    "model = model.to(settings.DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Check input shapes\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nInput shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudhamshuaddanki/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 0.8838\n",
      "NaN loss detected in batch 40\n",
      "Batch 100: Loss = 0.1925\n",
      "NaN loss detected in batch 127\n",
      "NaN loss detected in batch 155\n",
      "NaN loss detected in batch 166\n",
      "NaN loss detected in batch 174\n",
      "NaN loss detected in batch 188\n",
      "NaN loss detected in batch 195\n",
      "Batch 200: Loss = 0.1253\n",
      "Batch 300: Loss = 0.0838\n",
      "NaN loss detected in batch 314\n",
      "NaN loss detected in batch 316\n",
      "NaN loss detected in batch 324\n",
      "NaN loss detected in batch 379\n",
      "NaN loss detected in batch 399\n",
      "Batch 400: Loss = 0.0635\n",
      "NaN loss detected in batch 428\n",
      "NaN loss detected in batch 442\n",
      "Batch 500: Loss = 0.0538\n",
      "NaN loss detected in batch 511\n",
      "NaN loss detected in batch 524\n",
      "NaN loss detected in batch 527\n",
      "NaN loss detected in batch 543\n",
      "NaN loss detected in batch 551\n",
      "NaN loss detected in batch 573\n",
      "NaN loss detected in batch 576\n",
      "NaN loss detected in batch 581\n",
      "Batch 600: Loss = 0.0454\n",
      "NaN loss detected in batch 608\n",
      "NaN loss detected in batch 615\n",
      "NaN loss detected in batch 619\n",
      "NaN loss detected in batch 637\n",
      "NaN loss detected in batch 678\n",
      "Batch 700: Loss = 0.0403\n",
      "NaN loss detected in batch 714\n",
      "NaN loss detected in batch 765\n",
      "Batch 800: Loss = 0.0334\n",
      "NaN loss detected in batch 813\n",
      "NaN loss detected in batch 837\n",
      "NaN loss detected in batch 856\n",
      "Batch 900: Loss = 0.0298\n",
      "NaN loss detected in batch 962\n",
      "NaN loss detected in batch 985\n",
      "Batch 1000: Loss = 0.0264\n",
      "NaN loss detected in batch 1024\n",
      "NaN loss detected in batch 1051\n",
      "Batch 1100: Loss = 0.0232\n",
      "NaN loss detected in batch 1109\n",
      "NaN loss detected in batch 1123\n",
      "Epoch 1/10:\n",
      "Train Loss: 0.0879\n",
      "Val Loss: 0.0204\n",
      "Batch 0: Loss = 0.0221\n",
      "NaN loss detected in batch 2\n",
      "NaN loss detected in batch 45\n",
      "NaN loss detected in batch 48\n",
      "NaN loss detected in batch 82\n",
      "NaN loss detected in batch 87\n",
      "Batch 100: Loss = 0.0202\n",
      "NaN loss detected in batch 111\n",
      "NaN loss detected in batch 118\n",
      "NaN loss detected in batch 161\n",
      "Batch 200: Loss = 0.0177\n",
      "NaN loss detected in batch 284\n",
      "NaN loss detected in batch 295\n",
      "Batch 300: Loss = 0.0160\n",
      "NaN loss detected in batch 317\n",
      "Batch 400: Loss = 0.0145\n",
      "NaN loss detected in batch 474\n",
      "NaN loss detected in batch 499\n",
      "Batch 500: Loss = 0.0129\n",
      "NaN loss detected in batch 518\n",
      "Batch 600: Loss = 0.0116\n",
      "NaN loss detected in batch 603\n",
      "NaN loss detected in batch 607\n",
      "NaN loss detected in batch 657\n",
      "NaN loss detected in batch 658\n",
      "NaN loss detected in batch 671\n",
      "NaN loss detected in batch 677\n",
      "Batch 700: Loss = 0.0106\n",
      "NaN loss detected in batch 723\n",
      "NaN loss detected in batch 767\n",
      "NaN loss detected in batch 772\n",
      "NaN loss detected in batch 788\n",
      "Batch 800: Loss = 0.0096\n",
      "NaN loss detected in batch 813\n",
      "NaN loss detected in batch 873\n",
      "NaN loss detected in batch 876\n",
      "Batch 900: Loss = 0.0089\n",
      "NaN loss detected in batch 930\n",
      "NaN loss detected in batch 934\n",
      "NaN loss detected in batch 951\n",
      "NaN loss detected in batch 994\n",
      "Batch 1000: Loss = 0.0080\n",
      "NaN loss detected in batch 1020\n",
      "NaN loss detected in batch 1064\n",
      "NaN loss detected in batch 1081\n",
      "NaN loss detected in batch 1094\n",
      "Batch 1100: Loss = 0.0074\n",
      "NaN loss detected in batch 1106\n",
      "NaN loss detected in batch 1118\n",
      "NaN loss detected in batch 1126\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.0131\n",
      "Val Loss: 0.0068\n",
      "Batch 0: Loss = 0.0073\n",
      "NaN loss detected in batch 13\n",
      "NaN loss detected in batch 28\n",
      "NaN loss detected in batch 34\n",
      "NaN loss detected in batch 71\n",
      "Batch 100: Loss = 0.0068\n",
      "NaN loss detected in batch 115\n",
      "NaN loss detected in batch 120\n",
      "Batch 200: Loss = 0.0062\n",
      "NaN loss detected in batch 215\n",
      "NaN loss detected in batch 229\n",
      "NaN loss detected in batch 271\n",
      "Batch 300: Loss = 0.0057\n",
      "NaN loss detected in batch 304\n",
      "NaN loss detected in batch 317\n",
      "NaN loss detected in batch 333\n",
      "NaN loss detected in batch 383\n",
      "Batch 400: Loss = 0.0053\n",
      "NaN loss detected in batch 474\n",
      "NaN loss detected in batch 487\n",
      "Batch 500: Loss = 0.0048\n",
      "NaN loss detected in batch 506\n",
      "NaN loss detected in batch 543\n",
      "NaN loss detected in batch 544\n",
      "Batch 600: Loss = 0.0046\n",
      "NaN loss detected in batch 603\n",
      "NaN loss detected in batch 628\n",
      "NaN loss detected in batch 646\n",
      "NaN loss detected in batch 665\n",
      "NaN loss detected in batch 673\n",
      "NaN loss detected in batch 678\n",
      "NaN loss detected in batch 693\n",
      "Batch 700: Loss = 0.0042\n",
      "NaN loss detected in batch 725\n",
      "NaN loss detected in batch 764\n",
      "NaN loss detected in batch 769\n",
      "Batch 800: Loss = 0.0039\n",
      "NaN loss detected in batch 802\n",
      "NaN loss detected in batch 866\n",
      "NaN loss detected in batch 899\n",
      "Batch 900: Loss = 0.0036\n",
      "NaN loss detected in batch 960\n",
      "NaN loss detected in batch 990\n",
      "Batch 1000: Loss = 0.0034\n",
      "NaN loss detected in batch 1003\n",
      "NaN loss detected in batch 1016\n",
      "NaN loss detected in batch 1021\n",
      "NaN loss detected in batch 1044\n",
      "NaN loss detected in batch 1057\n",
      "Batch 1100: Loss = 0.0032\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.0048\n",
      "Val Loss: 0.0029\n",
      "Batch 0: Loss = 0.0031\n",
      "NaN loss detected in batch 9\n",
      "NaN loss detected in batch 13\n",
      "NaN loss detected in batch 31\n",
      "NaN loss detected in batch 35\n",
      "NaN loss detected in batch 42\n",
      "Batch 100: Loss = 0.0029\n",
      "NaN loss detected in batch 139\n",
      "NaN loss detected in batch 143\n",
      "NaN loss detected in batch 152\n",
      "NaN loss detected in batch 199\n",
      "Batch 200: Loss = 0.0027\n",
      "NaN loss detected in batch 222\n",
      "NaN loss detected in batch 266\n",
      "NaN loss detected in batch 279\n",
      "NaN loss detected in batch 296\n",
      "Batch 300: Loss = 0.0025\n",
      "NaN loss detected in batch 316\n",
      "NaN loss detected in batch 348\n",
      "Batch 400: Loss = 0.0023\n",
      "NaN loss detected in batch 441\n",
      "NaN loss detected in batch 482\n",
      "NaN loss detected in batch 492\n",
      "Batch 500: Loss = 0.0022\n",
      "NaN loss detected in batch 507\n",
      "NaN loss detected in batch 515\n",
      "NaN loss detected in batch 521\n",
      "NaN loss detected in batch 523\n",
      "NaN loss detected in batch 556\n",
      "NaN loss detected in batch 570\n",
      "Batch 600: Loss = 0.0020\n",
      "NaN loss detected in batch 678\n",
      "Batch 700: Loss = 0.0019\n",
      "NaN loss detected in batch 753\n",
      "NaN loss detected in batch 769\n",
      "NaN loss detected in batch 777\n",
      "Batch 800: Loss = 0.0019\n",
      "NaN loss detected in batch 869\n",
      "NaN loss detected in batch 879\n",
      "NaN loss detected in batch 886\n",
      "NaN loss detected in batch 890\n",
      "Batch 900: Loss = 0.0017\n",
      "NaN loss detected in batch 945\n",
      "NaN loss detected in batch 947\n",
      "NaN loss detected in batch 955\n",
      "NaN loss detected in batch 959\n",
      "Batch 1000: Loss = 0.0016\n",
      "NaN loss detected in batch 1095\n",
      "Batch 1100: Loss = 0.0015\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.0021\n",
      "Val Loss: 0.0014\n",
      "Batch 0: Loss = 0.0014\n",
      "NaN loss detected in batch 37\n",
      "NaN loss detected in batch 45\n",
      "NaN loss detected in batch 59\n",
      "Batch 100: Loss = 0.0013\n",
      "NaN loss detected in batch 114\n",
      "NaN loss detected in batch 186\n",
      "NaN loss detected in batch 195\n",
      "Batch 200: Loss = 0.0013\n",
      "NaN loss detected in batch 213\n",
      "NaN loss detected in batch 267\n",
      "Batch 300: Loss = 0.0012\n",
      "NaN loss detected in batch 309\n",
      "NaN loss detected in batch 337\n",
      "Batch 400: Loss = 0.0011\n",
      "NaN loss detected in batch 402\n",
      "NaN loss detected in batch 443\n",
      "Batch 500: Loss = 0.0011\n",
      "NaN loss detected in batch 522\n",
      "NaN loss detected in batch 525\n",
      "NaN loss detected in batch 539\n",
      "NaN loss detected in batch 544\n",
      "NaN loss detected in batch 560\n",
      "NaN loss detected in batch 568\n",
      "Batch 600: Loss = 0.0010\n",
      "NaN loss detected in batch 601\n",
      "NaN loss detected in batch 616\n",
      "NaN loss detected in batch 625\n",
      "NaN loss detected in batch 665\n",
      "NaN loss detected in batch 683\n",
      "Batch 700: Loss = 0.0009\n",
      "NaN loss detected in batch 734\n",
      "NaN loss detected in batch 736\n",
      "NaN loss detected in batch 767\n",
      "NaN loss detected in batch 796\n",
      "Batch 800: Loss = 0.0009\n",
      "NaN loss detected in batch 850\n",
      "NaN loss detected in batch 860\n",
      "NaN loss detected in batch 870\n",
      "NaN loss detected in batch 898\n",
      "Batch 900: Loss = 0.0008\n",
      "NaN loss detected in batch 917\n",
      "NaN loss detected in batch 926\n",
      "NaN loss detected in batch 933\n",
      "NaN loss detected in batch 955\n",
      "NaN loss detected in batch 994\n",
      "Batch 1000: Loss = 0.0008\n",
      "NaN loss detected in batch 1035\n",
      "NaN loss detected in batch 1042\n",
      "Batch 1100: Loss = 0.0007\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.0010\n",
      "Val Loss: 0.0007\n",
      "Batch 0: Loss = 0.0007\n",
      "NaN loss detected in batch 1\n",
      "NaN loss detected in batch 5\n",
      "NaN loss detected in batch 12\n",
      "NaN loss detected in batch 36\n",
      "Batch 100: Loss = 0.0007\n",
      "NaN loss detected in batch 110\n",
      "Batch 200: Loss = 0.0006\n",
      "NaN loss detected in batch 241\n",
      "Batch 300: Loss = 0.0006\n",
      "NaN loss detected in batch 375\n",
      "Batch 400: Loss = 0.0006\n",
      "NaN loss detected in batch 454\n",
      "NaN loss detected in batch 476\n",
      "NaN loss detected in batch 489\n",
      "NaN loss detected in batch 493\n",
      "Batch 500: Loss = 0.0005\n",
      "NaN loss detected in batch 523\n",
      "NaN loss detected in batch 530\n",
      "NaN loss detected in batch 546\n",
      "NaN loss detected in batch 550\n",
      "NaN loss detected in batch 556\n",
      "NaN loss detected in batch 589\n",
      "NaN loss detected in batch 592\n",
      "Batch 600: Loss = 0.0005\n",
      "NaN loss detected in batch 612\n",
      "NaN loss detected in batch 638\n",
      "NaN loss detected in batch 648\n",
      "NaN loss detected in batch 670\n",
      "Batch 700: Loss = 0.0005\n",
      "NaN loss detected in batch 705\n",
      "NaN loss detected in batch 751\n",
      "Batch 800: Loss = 0.0005\n",
      "NaN loss detected in batch 828\n",
      "NaN loss detected in batch 872\n",
      "NaN loss detected in batch 881\n",
      "NaN loss detected in batch 886\n",
      "Batch 900: Loss = 0.0004\n",
      "NaN loss detected in batch 929\n",
      "NaN loss detected in batch 950\n",
      "NaN loss detected in batch 973\n",
      "NaN loss detected in batch 993\n",
      "Batch 1000: Loss = 0.0004\n",
      "NaN loss detected in batch 1031\n",
      "NaN loss detected in batch 1040\n",
      "NaN loss detected in batch 1042\n",
      "NaN loss detected in batch 1087\n",
      "Batch 1100: Loss = 0.0004\n",
      "NaN loss detected in batch 1104\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0004\n",
      "Batch 0: Loss = 0.0004\n",
      "NaN loss detected in batch 22\n",
      "NaN loss detected in batch 35\n",
      "NaN loss detected in batch 41\n",
      "NaN loss detected in batch 47\n",
      "NaN loss detected in batch 48\n",
      "Batch 100: Loss = 0.0004\n",
      "NaN loss detected in batch 152\n",
      "NaN loss detected in batch 168\n",
      "NaN loss detected in batch 186\n",
      "Batch 200: Loss = 0.0003\n",
      "NaN loss detected in batch 212\n",
      "NaN loss detected in batch 267\n",
      "NaN loss detected in batch 286\n",
      "Batch 300: Loss = 0.0003\n",
      "NaN loss detected in batch 395\n",
      "Batch 400: Loss = 0.0003\n",
      "NaN loss detected in batch 419\n",
      "NaN loss detected in batch 426\n",
      "NaN loss detected in batch 429\n",
      "NaN loss detected in batch 442\n",
      "NaN loss detected in batch 477\n",
      "NaN loss detected in batch 494\n",
      "NaN loss detected in batch 498\n",
      "Batch 500: Loss = 0.0003\n",
      "NaN loss detected in batch 544\n",
      "NaN loss detected in batch 572\n",
      "NaN loss detected in batch 584\n",
      "Batch 600: Loss = 0.0003\n",
      "NaN loss detected in batch 605\n",
      "NaN loss detected in batch 666\n",
      "NaN loss detected in batch 693\n",
      "Batch 700: Loss = 0.0003\n",
      "NaN loss detected in batch 704\n",
      "NaN loss detected in batch 758\n",
      "NaN loss detected in batch 769\n",
      "NaN loss detected in batch 795\n",
      "Batch 800: Loss = 0.0002\n",
      "NaN loss detected in batch 868\n",
      "NaN loss detected in batch 885\n",
      "NaN loss detected in batch 887\n",
      "Batch 900: Loss = 0.0002\n",
      "NaN loss detected in batch 991\n",
      "Batch 1000: Loss = 0.0002\n",
      "NaN loss detected in batch 1016\n",
      "NaN loss detected in batch 1030\n",
      "NaN loss detected in batch 1034\n",
      "NaN loss detected in batch 1060\n",
      "NaN loss detected in batch 1071\n",
      "Batch 1100: Loss = 0.0002\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0002\n",
      "Batch 0: Loss = 0.0002\n",
      "NaN loss detected in batch 71\n",
      "Batch 100: Loss = 0.0002\n",
      "NaN loss detected in batch 106\n",
      "NaN loss detected in batch 114\n",
      "NaN loss detected in batch 118\n",
      "NaN loss detected in batch 139\n",
      "NaN loss detected in batch 147\n",
      "NaN loss detected in batch 159\n",
      "NaN loss detected in batch 189\n",
      "Batch 200: Loss = 0.0002\n",
      "NaN loss detected in batch 260\n",
      "NaN loss detected in batch 268\n",
      "Batch 300: Loss = 0.0002\n",
      "NaN loss detected in batch 315\n",
      "NaN loss detected in batch 372\n",
      "Batch 400: Loss = 0.0002\n",
      "NaN loss detected in batch 417\n",
      "NaN loss detected in batch 462\n",
      "Batch 500: Loss = 0.0002\n",
      "NaN loss detected in batch 512\n",
      "NaN loss detected in batch 561\n",
      "Batch 600: Loss = 0.0001\n",
      "NaN loss detected in batch 622\n",
      "NaN loss detected in batch 666\n",
      "NaN loss detected in batch 674\n",
      "NaN loss detected in batch 678\n",
      "NaN loss detected in batch 686\n",
      "NaN loss detected in batch 694\n",
      "Batch 700: Loss = 0.0001\n",
      "NaN loss detected in batch 707\n",
      "NaN loss detected in batch 712\n",
      "NaN loss detected in batch 716\n",
      "NaN loss detected in batch 719\n",
      "Batch 800: Loss = 0.0001\n",
      "NaN loss detected in batch 840\n",
      "NaN loss detected in batch 859\n",
      "Batch 900: Loss = 0.0001\n",
      "NaN loss detected in batch 905\n",
      "NaN loss detected in batch 906\n",
      "NaN loss detected in batch 936\n",
      "NaN loss detected in batch 970\n",
      "NaN loss detected in batch 979\n",
      "NaN loss detected in batch 986\n",
      "Batch 1000: Loss = 0.0001\n",
      "NaN loss detected in batch 1007\n",
      "NaN loss detected in batch 1022\n",
      "NaN loss detected in batch 1035\n",
      "Batch 1100: Loss = 0.0001\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.0002\n",
      "Val Loss: 0.0001\n",
      "Batch 0: Loss = 0.0001\n",
      "NaN loss detected in batch 28\n",
      "NaN loss detected in batch 66\n",
      "Batch 100: Loss = 0.0001\n",
      "NaN loss detected in batch 103\n",
      "NaN loss detected in batch 179\n",
      "Batch 200: Loss = 0.0001\n",
      "NaN loss detected in batch 242\n",
      "NaN loss detected in batch 253\n",
      "NaN loss detected in batch 255\n",
      "NaN loss detected in batch 284\n",
      "Batch 300: Loss = 0.0001\n",
      "NaN loss detected in batch 316\n",
      "NaN loss detected in batch 325\n",
      "Batch 400: Loss = 0.0001\n",
      "NaN loss detected in batch 401\n",
      "NaN loss detected in batch 419\n",
      "NaN loss detected in batch 479\n",
      "Batch 500: Loss = 0.0001\n",
      "NaN loss detected in batch 509\n",
      "NaN loss detected in batch 522\n",
      "NaN loss detected in batch 555\n",
      "NaN loss detected in batch 588\n",
      "Batch 600: Loss = 0.0001\n",
      "NaN loss detected in batch 646\n",
      "NaN loss detected in batch 684\n",
      "NaN loss detected in batch 691\n",
      "Batch 700: Loss = 0.0001\n",
      "NaN loss detected in batch 735\n",
      "NaN loss detected in batch 746\n",
      "NaN loss detected in batch 772\n",
      "Batch 800: Loss = 0.0001\n",
      "NaN loss detected in batch 856\n",
      "NaN loss detected in batch 868\n",
      "Batch 900: Loss = 0.0001\n",
      "NaN loss detected in batch 907\n",
      "NaN loss detected in batch 922\n",
      "NaN loss detected in batch 934\n",
      "NaN loss detected in batch 963\n",
      "NaN loss detected in batch 970\n",
      "Batch 1000: Loss = 0.0001\n",
      "NaN loss detected in batch 1018\n",
      "NaN loss detected in batch 1024\n",
      "NaN loss detected in batch 1030\n",
      "NaN loss detected in batch 1046\n",
      "NaN loss detected in batch 1047\n",
      "NaN loss detected in batch 1053\n",
      "Batch 1100: Loss = 0.0001\n",
      "NaN loss detected in batch 1126\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.0001\n",
      "Val Loss: 0.0001\n",
      "Batch 0: Loss = 0.0001\n",
      "NaN loss detected in batch 1\n",
      "NaN loss detected in batch 13\n",
      "NaN loss detected in batch 81\n",
      "NaN loss detected in batch 89\n",
      "Batch 100: Loss = 0.0001\n",
      "NaN loss detected in batch 109\n",
      "NaN loss detected in batch 112\n",
      "NaN loss detected in batch 133\n",
      "NaN loss detected in batch 193\n",
      "Batch 200: Loss = 0.0001\n",
      "NaN loss detected in batch 201\n",
      "NaN loss detected in batch 203\n",
      "NaN loss detected in batch 212\n",
      "NaN loss detected in batch 250\n",
      "NaN loss detected in batch 277\n",
      "NaN loss detected in batch 287\n",
      "Batch 300: Loss = 0.0001\n",
      "NaN loss detected in batch 342\n",
      "NaN loss detected in batch 369\n",
      "Batch 400: Loss = 0.0001\n",
      "NaN loss detected in batch 433\n",
      "NaN loss detected in batch 444\n",
      "NaN loss detected in batch 459\n",
      "NaN loss detected in batch 485\n",
      "Batch 500: Loss = 0.0000\n",
      "NaN loss detected in batch 507\n",
      "Batch 600: Loss = 0.0000\n",
      "NaN loss detected in batch 613\n",
      "NaN loss detected in batch 615\n",
      "NaN loss detected in batch 630\n",
      "NaN loss detected in batch 632\n",
      "NaN loss detected in batch 665\n",
      "NaN loss detected in batch 684\n",
      "Batch 700: Loss = 0.0000\n",
      "NaN loss detected in batch 701\n",
      "NaN loss detected in batch 731\n",
      "NaN loss detected in batch 759\n",
      "NaN loss detected in batch 777\n",
      "Batch 800: Loss = 0.0000\n",
      "NaN loss detected in batch 817\n",
      "NaN loss detected in batch 865\n",
      "Batch 900: Loss = 0.0000\n",
      "NaN loss detected in batch 928\n",
      "NaN loss detected in batch 971\n",
      "NaN loss detected in batch 986\n",
      "Batch 1000: Loss = 0.0000\n",
      "NaN loss detected in batch 1006\n",
      "NaN loss detected in batch 1081\n",
      "Batch 1100: Loss = 0.0000\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.0000\n",
      "Val Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BetslipRecommenderModel(\n",
       "  (user_tower): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (event_tower): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (market_tower): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (predictor): Sequential(\n",
       "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "train_betslip_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations...\n",
      "Preparing to generate recommendations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating recommendations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_betslip_recommendations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with actual user ID\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavailable_events\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarket_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarket_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recommendations:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop Recommendations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[186], line 37\u001b[0m, in \u001b[0;36mgenerate_betslip_recommendations\u001b[0;34m(model, user_id, available_events, market_df, dataset, top_k, max_selections, min_odds, max_combined_odds)\u001b[0m\n\u001b[1;32m     34\u001b[0m market_features[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmarket_features[market_idx]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Get odds\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m odds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mmarket_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmarket_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarket_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmarket_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_odds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m odds \u001b[38;5;241m<\u001b[39m min_odds:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1089\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1412\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m-> 1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getbool_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n",
      "File \u001b[0;32m~/smaddanki/Assessments/Whizdomai/.venv/lib/python3.13/site-packages/pandas/core/common.py:97\u001b[0m, in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     93\u001b[0m             name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_bool_indexer\u001b[39m(key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Check whether `key` is a valid boolean indexer.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        and convert to an ndarray.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    126\u001b[0m         key, (ABCSeries, np\u001b[38;5;241m.\u001b[39mndarray, ABCIndex, ABCExtensionArray)\n\u001b[1;32m    127\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ABCMultiIndex):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "print(\"Generating recommendations...\")\n",
    "recommendations = generate_betslip_recommendations(\n",
    "    model=model,\n",
    "    user_id=101,  # Replace with actual user ID\n",
    "    available_events=event_df['event_id'].tolist(),\n",
    "    market_df=market_df,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "if recommendations:\n",
    "    print(\"\\nTop Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        events_info, markets_info = get_betslip_details(rec, event_df, market_df)\n",
    "        print(f\"\\nBetslip Recommendation {i}:\")\n",
    "        for j, (event, market, odd) in enumerate(zip(events_info, markets_info, rec['odds'])):\n",
    "            print(f\"Selection {j+1}:\")\n",
    "            print(f\"  Event: {event}\")\n",
    "            print(f\"  Market: {market}\")\n",
    "            print(f\"  Odds: {odd:.2f}\")\n",
    "        print(f\"Combined Odds: {rec['combined_odds']:.2f}\")\n",
    "        print(f\"Confidence Score: {rec['score']:.4f}\")\n",
    "else:\n",
    "    print(\"No recommendations generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
